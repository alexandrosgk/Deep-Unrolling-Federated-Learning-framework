{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64733,"status":"ok","timestamp":1669927364772,"user":{"displayName":"Alexandros Gkillas","userId":"06090391217186072891"},"user_tz":-120},"id":"L6m7JN-ogIrq","outputId":"487bb6cc-934a-4bb7-e175-63178aa38272"},"outputs":[],"source":["#@title Import necessary libraries\n","\n","import os\n","import numpy as np\n","import random\n","import pandas as pd\n","import torch\n","import copy\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from torch.utils.data import DataLoader, TensorDataset, Dataset\n","from torch.nn import Linear, ReLU, LeakyReLU, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from torchvision import datasets, transforms\n","from math import sqrt\n","from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","from matplotlib import pyplot as plt\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.autograd as autograd\n","import copy\n","# from tensorflow import keras\n","import thread6 as thread\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#@title deep unrolling training\n","\n","# Function to create a downsampling operator for an image\n","def downsampling_operator(s):\n","    # Initialize an empty matrix for the downsampled image\n","    S = np.zeros((np.int_(64/s), 64))\n","    vector = np.zeros((1, 64))\n","    vector[0,0] = 1\n","\n","    # Loop over the downsampled image indices and create the downsampled image by shifting the elements in the vector\n","    for i in range(np.int_(64/s)):\n","        S[i,:] = vector\n","        vector = np.roll(vector, s, axis=1)\n","    # Return as a PyTorch tensor\n","    return torch.from_numpy(S).float()\n","\n","# Function to count the number of trainable parameters in a PyTorch model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Function to add Gaussian noise to the training data, used for data augmentation\n","def add_noise_to_training_data(training_data_pred_ground_truth, noise_sensor=0.03):\n","    # Initialize an empty array to hold the noisy data\n","    training_data_pred_noisy = np.zeros(training_data_pred_ground_truth.shape)\n","    # Loop over the training data\n","    for i in range(training_data_pred_ground_truth.shape[0]):\n","        gt = training_data_pred_ground_truth[i,:,:,:]\n","        # Generate Gaussian noise with mean 0 and standard deviation equal to sensor_noise\n","        noise = np.random.normal(0, sensor_noise, gt.shape) # mu, sigma, size\n","        # If the ground truth is zero, make the noise zero as well\n","        noise[gt == 0] = 0\n","        # Add the noise to the ground truth\n","        training_data_pred_noisy[i,:,:,:] = gt + noise\n","    return training_data_pred_noisy\n","\n","# Function to pretrain the DnCNN model\n","def pretraining(DnCnn_model, num_epochs, learning_rate, dataloader_pretrain, device, l2_w, batch_size):\n","    # Define the loss function (L1 loss)\n","    criterion = nn.L1Loss()\n","    # Define the optimizer (Adam optimizer with weight decay)\n","    optimizer = torch.optim.Adam(DnCnn_model.parameters(), lr=learning_rate, weight_decay=l2_w)\n","\n","    # Loop over the epochs\n","    for epoch in range(num_epochs):\n","        acc_loss = 0.\n","        # Loop over the batches in the pretraining dataloader\n","        for data_noisy, clean in dataloader_pretrain:\n","            # If batch size is greater than 1, adjust it to the size of the current batch\n","            if batch_size>1:\n","                batch_size = len(data_noisy)           \n","            # Move data and labels to the device (GPU or CPU)\n","            data = (data_noisy.float()).to(device)\n","            clean = (clean.float()).to(device)\n","            # Rearrange the dimensions of the data and labels to match the DnCNN input\n","            data = torch.moveaxis(data, 3, 2)\n","            data = torch.moveaxis(data, 2, 1)\n","            clean = torch.moveaxis(clean, 3, 2)\n","            clean = torch.moveaxis(clean, 2, 1)\n","            # Forward pass: Compute predicted y by passing x to the model\n","            output = DnCnn_model(data)\n","            # Compute loss\n","            loss = torch.sqrt(criterion(output, clean))\n","            # Zero gradients, perform a backward pass, and update the weights\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            # Accumulate the loss\n","            acc_loss += loss.item()\n","        # Print the average loss for this epoch\n","        print(acc_loss / len(dataloader_pretrain))\n","        \n","# Function to train the Deep unrolling model\n","def unrolling_model_training(my_sc_model, num_epochs, learning_rate, dataloader_train, device, l2_w, batch_size, S, blocks):\n","    # Set the model to training mode\n","    my_sc_model.train()\n","    # Define the loss function (L1 loss)\n","    criterion = nn.L1Loss()\n","    # Define the optimizer (Adam optimizer)\n","    optimizer = torch.optim.Adam(my_sc_model.parameters(), lr=learning_rate)\n","\n","    # Loop over the epochs\n","    for epoch in range(num_epochs):\n","       acc_loss = 0.\n","       # Loop over the batches in the training dataloader\n","       for data, clean in dataloader_train:\n","           # Adjust batch size to the size of the current batch\n","           batch_size = len(data)\n","           # Move data and labels to the device (GPU or CPU)\n","           data = (data.float()).to(device)\n","           clean = (clean.float()).to(device)\n","           # Rearrange the dimensions of the data and labels to match the model's input\n","           data = torch.moveaxis(data, 3, 2)\n","           data = torch.moveaxis(data, 2, 1)\n","           clean = torch.moveaxis(clean, 3, 2)\n","           clean = torch.moveaxis(clean, 2, 1)\n","           # Forward pass: Compute predicted y by passing x to the model\n","           output = my_sc_model(data, S, batch_size, device, blocks)  \n","           # Compute loss\n","           loss = criterion(output, clean)\n","           # Zero gradients, perform a backward pass, and update the weights\n","           optimizer.zero_grad()\n","           loss.backward()\n","           optimizer.step()\n","           # Accumulate the loss\n","           acc_loss += loss.item()\n","       # Print the average loss for this epoch\n","       print(acc_loss / len(dataloader_train))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#@title Deep unrolling architecture\n","\n","# Function to enable dropout layers during testing phase\n","def enable_dropout(model):\n","    \"\"\" \n","    This function is used to enable dropout layers during testing phase. Dropout is typically\n","    only used during training, and this can help to create a more robust model.\n","    \"\"\"\n","    for m in model.modules():\n","        if m.__class__.__name__.startswith('Dropout'):\n","            m.train()\n","\n","# Define a DnCNN network (a deep learning model for image denoising)\n","class DnCnn_net(nn.Module):\n","       def __init__(self, num_of_layers, features, input_channels):\n","         \"\"\"\n","         Initialize the DnCnn_net model with given number of layers, features, and input channels.\n","         \"\"\"\n","         super().__init__()\n","         kernel_size = 3  # Kernel size for the convolutional layers\n","         padding = 1  # Padding for the convolutional layers\n","         features = features  # Number of feature maps\n","         channels = input_channels  # Number of input channels\n","         num_of_layers = num_of_layers  # Number of layers in the network\n","         layers = []  # List to hold the layers\n","\n","         # First layer of the network (Convolution + ReLU + Dropout)\n","         layers.append(nn.utils.spectral_norm(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False)))\n","         layers.append(nn.ReLU())  \n","         layers.append(nn.Dropout(0.05))\n","\n","         # Intermediate layers (Convolution + ReLU + Dropout)\n","         for _ in range(num_of_layers-2):\n","             layers.append(nn.utils.spectral_norm(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False)))\n","             layers.append(nn.ReLU())   \n","             layers.append(nn.Dropout(0.05))\n","\n","         # Final layer of the network (Convolution)\n","         layers.append(nn.utils.spectral_norm(nn.Conv2d(in_channels=features, out_channels=1, kernel_size=kernel_size, padding=padding, bias=False)))\n","         \n","         # Create the model by putting all the layers in a sequential order\n","         self.dncnn = (nn.Sequential(*layers))\n","        \n","       def forward(self, input):\n","           \"\"\"\n","           Implement the forward pass of the network. This is called when we pass data through the model.\n","           \"\"\"\n","           return self.dncnn(input) \n","       \n","\n","# Define a deep unrolled sparse coding model\n","class deep_unrolled_sc(Module):   \n","    def __init__(self, DnCnn):\n","        super(deep_unrolled_sc, self).__init__()\n","        self.DnCnn = DnCnn\n","        self.alpha=nn.Parameter(torch.tensor(10.99), requires_grad=True)\n","        # self.DtD_approx = nn.Parameter(torch.rand(64, 64), requires_grad=True)\n","\n","    def forward(self, data, S, batch_size, device, blocks):\n","        low_dim = S.shape[0]\n","        high_dim = S.shape[1]        \n","        \n","        # Use broadcasting instead of explicit repetition\n","        S = S.unsqueeze(0).expand(batch_size, -1, -1).to(device)\n","        II = torch.eye(S.shape[2]).unsqueeze(0).expand(batch_size, -1, -1, -1).to(device)\n","\n","        # Use batched matrix multiplication\n","        STS = torch.bmm(S.transpose(1, 2), S)\n","        STS = STS.unsqueeze(dim=1).to(device)\n","        \n","\n","        A = torch.matmul(S.transpose(1, 2).unsqueeze(dim=1), data)\n","        A = self.alpha * A\n","        \n","        DtD = torch.linalg.inv(II + STS * self.alpha)\n","        # DtD = self.DtD_approx.unsqueeze(0).unsqueeze(0).expand(batch_size, -1, -1, -1)\n","\n","        # Initialize Z using zeros instead of ones\n","        Z = torch.zeros(batch_size, 1, high_dim, data.shape[3]).to(device)\n","\n","        for kk in range(blocks):\n","            x = torch.matmul(DtD, (A + Z))\n","            Z = self.DnCnn(x)\n","\n","        return Z"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ouster_range_image\n","##################################################\n","Input image resolution:   16 by 1024\n","Output image resolution:  64 by 1024\n","Upscaling ratio:          4\n","##################################################\n","Sensor minimum range:     2.0\n","Sensor maximum range:     80.0\n","Sensor view angle:        -16.6 to 16.6\n","Range normalize ratio:    100.0\n","##################################################\n","Training data-set:        /home/alexgi/Workspace/lidar_super_resolution/Lidar Super-Resolution/carla_ouster_range_image.npy \n","Testing data-set:         /home/alexgi/Workspace/lidar_super_resolution/Lidar Super-Resolution/ouster_range_image.npy\n","add noise level: 0.03\n"]}],"source":["#@title Read the training data\n","\n","\n","# Import helper functions to load data from the 'data_paper' module\n","from data_paper import *\n","\n","# Load training data\n","# training_data_input: The input data for training\n","# training_data_pred_ground_truth: The corresponding ground truth data\n","training_data_input, training_data_pred_ground_truth = load_train_data()\n","\n","# Create a TensorDataset from the training data.\n","train_ds = TensorDataset(torch.from_numpy(training_data_input), torch.from_numpy(training_data_pred_ground_truth))\n","\n","# Define the batch size for training\n","batch_size = 6\n","dataloader_train = DataLoader(train_ds, batch_size, shuffle=True)\n","\n","# Initialize the downsampling operator 'S' by calling the defined function with the downsampling factor 4.\n","S = downsampling_operator(4)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters: 115841\n","2.0038869713768364\n","0.09332608881592751\n","0.09204575098305941\n","0.09076253075152635\n","0.08726141490414739\n","0.0776420999802649\n","0.06597186245769263\n","0.05393738604336977\n","0.04388619832880795\n","0.03680832852423191\n","0.03273154521547258\n","0.02946196016855538\n","0.026508570382371546\n","0.025015232061967253\n","0.023426709088496863\n","0.02185383191332221\n","0.02083214063383639\n","0.01946202701330185\n","0.018196363140828908\n","0.0171221939381212\n","0.016140389575622974\n","0.015639372297562657\n","0.015174192999489605\n","0.014782508102245628\n","0.014476693000644445\n","0.01415575338806957\n","0.013878646411001682\n","0.013884387313388287\n","0.013665334686636926\n","0.013269385248422623\n","0.013186699378304183\n","0.013156322318129242\n","0.012910316251218319\n","0.012807581613305957\n","0.012720054478384554\n","0.012634680661838501\n","0.012645330363884568\n","0.012529811615124346\n","0.012499766464345157\n","0.012486009208485483\n","0.012359317543450743\n","0.012309590389020741\n","0.012245889580342919\n","0.012222120181657374\n","0.012212040639016777\n","0.012146015568170696\n","0.012142755012027918\n","0.012093015253543853\n","0.012133112494833767\n","0.012071358671411871\n","0.011795451265294105\n","0.011773325973656028\n","0.0117591218855232\n","0.011744221342727542\n","0.011733674352988601\n","0.01172359410719946\n","0.011714078769087791\n","0.011703046788461506\n","0.011694581037852913\n","0.011685240562073887\n","0.011676964748185128\n","0.011666975458152592\n","0.011658269078936427\n","0.01165143569931388\n","0.011642830733209849\n","0.01163672899780795\n","0.011627640388906003\n","0.011620992281008512\n","0.011616989683359861\n","0.011606946290004999\n","0.011601804343983531\n","0.011595010304357856\n","0.011588877593632788\n","0.011583454943727702\n","0.011574944434687495\n","0.011569956003222614\n","0.011565183360595256\n","0.011558703795541077\n","0.01155266796424985\n","0.011546107266563922\n","0.011542007544077932\n","0.011536450315266848\n","0.011531490679364651\n","0.011526994982268661\n","0.011520335712004453\n","0.011516772777307779\n","0.011512508617714048\n","0.01150794252101332\n","0.011500828295946121\n","0.011495565673336386\n","0.011492072957102209\n","0.011485143727622927\n","0.011480722738895565\n","0.011477812769357115\n","0.011474508717190475\n","0.011468565900810064\n","0.011462514458224177\n","0.011459343646187335\n","0.011455808470491319\n","0.011451101342681796\n"]}],"source":["#@title Train the deep unrolling model\n","\n","# Initialize the DnCnn model with 5 layers, 64 features, and 1 input channel\n","# This model will be used as part of the deep unrolled super-resolution model to act as denoiser-regularizer\n","DnCnn_model = DnCnn_net(5, 64, 1).to(device)\n","\n","# Initialize the deep unrolled super-resolution (sc) model with the DnCnn model\n","# This model will be used for training on the super-resolution task\n","my_sc_model = deep_unrolled_sc(DnCnn_model).cuda()\n","\n","# Calculate the total number of trainable parameters in the super-resolution model\n","# This information is useful for understanding the capacity of the model\n","num_of_param = count_parameters(my_sc_model)\n","print('Number of parameters:', num_of_param)\n","\n","# Set the learning rate for the Adam optimizer\n","learning_rate = 1e-3\n","# Set the number of unrolling blocks for the deep unrolled model\n","blocks  = 6\n","# Set the number of epochs for training the model\n","num_epochs = 50\n","# Set the L2 regularization weight for preventing overfitting\n","l2_w = 1e-6\n","\n","# Train the deep unrolled model with the specified parameters\n","unrolling_model_training(my_sc_model, num_epochs, learning_rate, dataloader_train, \n","                         device, l2_w, batch_size, \n","                         S, blocks)\n","\n","# After the initial round of training, decrease the learning rate to refine the parameters\n","learning_rate = 1e-4\n","\n","# Continue training the deep unrolled model with the lowered learning rate\n","unrolling_model_training(my_sc_model, num_epochs, learning_rate, dataloader_train, \n","                         device, l2_w, batch_size, \n","                         S, blocks)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["L1 error:  0.022618724518526798\n"]}],"source":["#@title Test\n","\n","# Import the necessary functions from your data handling module\n","from data_paper import *\n","\n","# Load the test data inputs and ground truth\n","test_data_input, test_data = load_test_data()\n","\n","# Initialize a list to hold the L1 loss for each test image\n","l1_error = []\n","\n","# Initialize an array to hold the predicted range images\n","predicted_range_images = np.zeros_like(test_data)\n","\n","# Loop over each image in the test set\n","for i in range(0, test_data.shape[0]):\n","    # Extract the low resolution and high resolution images for this test case\n","    low_res = test_data_input[i,:,:,:]\n","    high_res = test_data[i,:,:,:]\n","\n","    # Convert these numpy arrays to PyTorch tensors and move them to the GPU\n","    low_res = torch.from_numpy(low_res).to(device)\n","    high_res = torch.from_numpy(high_res).to(device)\n","\n","    # Add a new dimension to these tensors to indicate the batch size\n","    low_res = torch.unsqueeze(low_res, dim=0)\n","    high_res = torch.unsqueeze(high_res, dim=0)\n","\n","    # Change the order of the dimensions to be compatible with PyTorch's expectations\n","    low_res = torch.moveaxis(low_res, 3, 2)\n","    low_res = torch.moveaxis(low_res, 2, 1)\n","    high_res = torch.moveaxis(high_res, 3, 2)\n","    high_res = torch.moveaxis(high_res, 2, 1)\n","\n","    # Feed the low resolution image into the model to get the high resolution prediction\n","    output = my_sc_model(low_res.float(), S, 1, device, blocks)  \n","\n","    # Clamp the output values to be between 0 and 1\n","    output[output < 0] = 0\n","    output[output > 1] = 1\n","    \n","    # Detach the output tensor from the computation graph and move it to the CPU\n","    high = output[0,0:,:,:].cpu().detach().numpy()\n","    high = np.squeeze(high, axis=0)\n","    \n","    # Store this high resolution prediction in the appropriate place in the predicted_range_images array\n","    predicted_range_images[i,:,:,0] = high\n","\n","    # Calculate the L1 loss between the high resolution prediction and ground truth\n","    criterion = nn.L1Loss()\n","    l1_error.append(criterion(output, high_res).item())\n","\n","# Print the average L1 loss across all the test images\n","print('L1 error: ', np.mean(l1_error))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#@title Define the Monte Carlo Dropout function\n","def MC_drop(test_data_input, iterate_count=10):\n","\n","    # Initialize arrays to hold temporary and final results\n","    this_test = np.empty([iterate_count, image_rows_low, image_cols, channel_num], dtype=np.float32)\n","    test_data_prediction = np.empty([test_data_input.shape[0], image_rows_high, image_cols, 2], dtype=np.float32)\n","    \n","    # Set the model to evaluation mode and enable dropout\n","    my_sc_model.eval()\n","    enable_dropout(my_sc_model)\n","\n","    # Loop over each image in the input data\n","    for i in range(test_data_input.shape[0]):\n","        print('Processing {} th of {} images ... '.format(i, test_data_prediction.shape[0]))\n","        \n","        # Initialize an array to hold the predictions for this image\n","        this_prediction = torch.zeros(iterate_count, 1, 64, 1024)\n","\n","        # Perform `iterate_count` forward passes with dropout\n","        for j in range(iterate_count):\n","            this_test = test_data_input[i]\n","            low_res = torch.from_numpy(this_test).to(device)\n","            low_res = torch.unsqueeze(low_res, dim=0)\n","            low_res = torch.moveaxis(low_res, 3, 2)\n","            low_res = torch.moveaxis(low_res, 2, 1)\n","            ooo = my_sc_model(low_res.float(), S, 1, device, blocks)\n","            this_prediction[j] = ooo.cpu().detach()\n","\n","        # Rearrange dimensions to match expectation\n","        this_prediction = torch.moveaxis(this_prediction, 2, 1)\n","        this_prediction = torch.moveaxis(this_prediction, 3, 2)\n","\n","        # Convert to numpy array\n","        this_prediction = this_prediction.cpu().detach().numpy()\n","\n","        # Compute the mean and standard deviation of the predictions\n","        this_prediction_mean = np.mean(this_prediction, axis=0)\n","        this_prediction_var = np.std(this_prediction, axis=0)\n","\n","        # Store the mean and variance in the final results array\n","        test_data_prediction[i,:,:,0:1] = this_prediction_mean\n","        test_data_prediction[i,:,:,1:2] = this_prediction_var\n","\n","    # Return the final results array\n","    return test_data_prediction"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["#@title Testing Range Images \n","\n","# Load the test data input (ignoring ground truth data with `_`)\n","test_data_input, _ = load_test_data()\n","\n","# Call the MC_drop function on your test data input, requesting 50 iterations for each sample\n","# This will provide an array of mean and variance for each test sample's prediction\n","predImages = MC_drop(test_data_input, iterate_count=50)\n","\n","# Rescale the predicted images by multiplying by 100 (this may be specific to your particular application or data)\n","predImages = predImages *100\n","\n","# Save the prediction results as a numpy array in a .npy file for future use or analysis\n","np.save('predImages_6blocks_test_June.npy', predImages)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMuEIv/063pxg6PEZ3bA5Fc","machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.8 ('mytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"fce9eb01ad10780b2b8a2e0f9ce538ecc07515bbabb4a0f9979fd114ae1e68b6"}}},"nbformat":4,"nbformat_minor":0}
